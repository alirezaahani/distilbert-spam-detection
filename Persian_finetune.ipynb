{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -U datasets huggingface_hub transformers[torch] evaluate --quiet\r\n",
        "\r\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset, ClassLabel, load_from_disk, load_metric\r\n",
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AutoTokenizer\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import transformers\r\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "UfEizXsw9IB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def clean(batch):\r\n",
        "  batch['text'] = [' '.join(text.split()) if text else '' for text in batch['text']]\r\n",
        "  return batch\r\n",
        "\r\n",
        "def tokenize(batch):\r\n",
        "  return tokenizer(batch[\"text\"], truncation=True, max_length=512)\r\n",
        "\r\n",
        "def adjust_labels(batch):\r\n",
        "  batch['label_'] = [1 if label == 'spam' else 0 for label in batch['label']]\r\n",
        "  return batch\r\n",
        "\r\n",
        "def str_int_labels(batch):\r\n",
        "  batch['label_'] = int(batch['label'])\r\n",
        "  return batch\r\n",
        "\r\n",
        "def all_ham(batch):\r\n",
        "  batch['label'] = 0\r\n",
        "  return batch\r\n",
        "\r\n",
        "def all_spam(batch):\r\n",
        "  batch['label'] = 1\r\n",
        "  return batch\r\n",
        "\r\n",
        "MAX_WORDS = 250\r\n",
        "\r\n",
        "def chunk_dataset(example):\r\n",
        "  example['text'] = example['text'].split()\r\n",
        "  example['text'] = [example['text'][i:i+MAX_WORDS] for i in range(0, len(example['text']), MAX_WORDS) if len(example['text'][i:i+MAX_WORDS]) > 3]\r\n",
        "  example['text'] = [' '.join(x) for x in example['text']]\r\n",
        "  return example\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "persian_alpha_codepoints = '\\u0621-\\u0628\\u062A-\\u063A\\u0641-\\u0642\\u0644-\\u0648\\u064E-\\u0651\\u0655\\u067E\\u0686\\u0698\\u06A9\\u06AF\\u06BE\\u06CC'\r\n",
        "\r\n",
        "PERSIAN_PATTERN = re.compile('['+persian_alpha_codepoints+']')\r\n",
        "\r\n",
        "def is_persian(example):\r\n",
        "  example['is_persian'] = bool(PERSIAN_PATTERN.search(example['text']))\r\n",
        "  return example"
      ],
      "outputs": [],
      "metadata": {
        "id": "1D73F4uT9MgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "persian_blog = (\n",
        "    load_dataset(\"RohanAiLab/persian_blog\")\n",
        "    .map(all_ham, batched=False)\n",
        ")\n",
        "\n",
        "persian_daily_news = (\n",
        "    load_dataset(\"RohanAiLab/persian_daily_news\")\n",
        "    .map(all_ham, batched=False)\n",
        ")\n",
        "\n",
        "all_datasets = concatenate_datasets([\n",
        "  persian_blog['train'],\n",
        "  persian_daily_news['train']\n",
        "])\n",
        "\n",
        "chunked_ = all_datasets.filter(lambda x: x['text'], batched=False)#.map(chunk_dataset, batched=False)\n",
        "dataset = Dataset.from_pandas(chunked_.to_pandas().explode('text'), preserve_index=False)\n",
        "dataset = dataset.shuffle(seed=49).filter(lambda x: x['text'], batched=False)\n",
        "#dataset.save_to_disk('/content/drive/MyDrive/Spam detection/PersianBlog')"
      ],
      "outputs": [],
      "metadata": {
        "id": "E0IiFPMD9Sre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Spam detection/Model')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/Spam detection/Model', num_labels=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dI81_4qqAKTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = evaluate.load(\"accuracy\")\n",
        "   load_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "outputs": [],
      "metadata": {
        "id": "qpwNy2vtAY63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = dataset.map(clean, batched=True).map(tokenize, batched=True).train_test_split(0.3)\n",
        "\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ],
      "outputs": [],
      "metadata": {
        "id": "9vlCTxIRAduf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "training_args = TrainingArguments(\n",
        "   output_dir=\"./Model\" ,\n",
        "   learning_rate=5e-6,\n",
        "   num_train_epochs=1,\n",
        "   weight_decay=0.001,\n",
        "   per_device_train_batch_size=64,\n",
        "   per_device_eval_batch_size=64,\n",
        "   dataloader_num_workers=2,\n",
        "   fp16=True,\n",
        "   warmup_ratio=0.3,\n",
        "   evaluation_strategy='steps',\n",
        "   save_total_limit=2,\n",
        "   save_steps=0.1,\n",
        "   eval_steps=1/3,\n",
        "   resume_from_checkpoint=True,\n",
        "   report_to='none',\n",
        "   label_smoothing_factor=0.2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train,\n",
        "   eval_dataset=test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "-rWkkxNjAOO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.train()"
      ],
      "outputs": [],
      "metadata": {
        "id": "3G8cKPi6ASdX"
      }
    }
  ]
}